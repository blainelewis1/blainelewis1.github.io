{"componentChunkName":"component---src-templates-blog-js","path":"/blog/bandit/","result":{"data":{"mdx":{"id":"e40bdef4-6784-5636-8359-089e7bac06d4","excerpt":"This page isn't really intended for your learning, instead this is my attempt to understand how bandits work. I hope it also helps youâ€¦","body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"\\\"Pull the Lever Kronk!\\\"\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"This page isn't really intended for your learning, instead this is my attempt to understand how bandits work. I hope it also helps you.\")), mdx(\"p\", null, \"Bandits are levers you pull to get some reward. In this post, we'll talk about Bernoulli bandits which are special bandits that yield either a 1 or 0, or a success or a failure.\"), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://media.giphy.com/media/14rfBl2RMAHtCM/giphy.gif\",\n    \"alt\": \"Emperor's new groove, pull the lever Kronk\"\n  }))), mdx(AnimatedBandit, {\n    graphs: false,\n    history: false,\n    pullLots: false,\n    controls: false,\n    bandits: [randomBernoulli(0.8)],\n    mdxType: \"AnimatedBandit\"\n  }), mdx(\"p\", null, \"The distribution underlying a bandit isn't necessarily uniform, as seen above. And you can put multiple bandits together, each with a different underlying reward function.\"), mdx(AnimatedBandit, {\n    graphs: false,\n    controls: false,\n    bandits: [randomBernoulli(0.6), randomBernoulli(0.4)],\n    mdxType: \"AnimatedBandit\"\n  }), mdx(\"p\", null, \"It then becomes difficult to know which bandit to pick.\"), mdx(\"p\", null, mdx(\"img\", _extends({\n    parentName: \"p\"\n  }, {\n    \"src\": \"https://media.giphy.com/media/11Pl7vl0Feh2Cc/giphy.gif\",\n    \"alt\": \"Emperor's new groove, wrong lever Kronk!\"\n  }))), mdx(\"p\", null, \"There is a tradeoff, every time you pull a bandit, you learn a little bit more about that bandit, but you are also missing out on pulling another bandit. This is the \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"explore\"), \" and \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"exploit\"), \" tradeoff.\"), mdx(\"h3\", null, \"Epsilon Greedy\"), mdx(\"p\", null, \"There are however algorithms for dealing with this. The most simple one says that for most lever pulls we will choose the arm that we calculate to give the highest average reward. But every once in a while we will explore and try a random lever.\"), mdx(AnimatedBandit, {\n    bandits: [randomBernoulli(0.2), randomBernoulli(0.8)],\n    algorithm: epsilonGreedy(),\n    mdxType: \"AnimatedBandit\"\n  }), mdx(\"h3\", null, \"UCB\"), mdx(\"p\", null, \"But this has many problems, one of the bigger problems is that after many iterations we no longer need to explore. That's where UCB comes in. UCB says, let's favour bandits with higher average reward, but also understand that for bandits without many pulls we should explore them once in a while.\"), mdx(AnimatedBandit, {\n    bandits: [randomBernoulli(0.2), randomBernoulli(0.8)],\n    algorithm: ucb,\n    mdxType: \"AnimatedBandit\"\n  }), mdx(\"h3\", null, \"Thompson Sampling\"), mdx(\"p\", null, \"A final approach is based on Bayesian statistics. Rather than modelling what we think the estimate is, we also model how confident we are in that estimate by sampling from a beta distribution that models the bandit as a prior. As we get more samples we become more confident.\"), mdx(AnimatedBandit, {\n    bandits: [randomBernoulli(0.2), randomBernoulli(0.8)],\n    algorithm: thompsonSampling,\n    mdxType: \"AnimatedBandit\"\n  }), mdx(AnimatedBandit, {\n    bandits: [randomBernoulli(0.4), randomBernoulli(0.6), randomBernoulli(0.1)],\n    algorithm: thompsonSampling,\n    mdxType: \"AnimatedBandit\"\n  }), mdx(\"h1\", null, \"Attributions\"), mdx(\"p\", null, \"lever by Deemak Daksina from the Noun Project\"));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"title":"\"Pull the Lever Kronk!\""},"fields":{"slug":"blog/bandit/"}}},"pageContext":{"id":"e40bdef4-6784-5636-8359-089e7bac06d4"}},"staticQueryHashes":["4176149245"]}